{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Assignment 1.2\n",
        "## Practical Deep Learning for Language Processing\n",
        "\n",
        "11/27/2022\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"brown.txt\", \"r\") as file:\n",
        "    brown_corpus = file.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def get_word_frequencies(corpus):\n",
        "    # get words and their frequencies for the split corpus\n",
        "    words, frequencies = np.unique(corpus.split(), return_counts = True)\n",
        "    # return a series with the words as indices\n",
        "    return pd.Series(frequencies, index = words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "County    1\n",
              "Friday    1\n",
              "Fulton    1\n",
              "Grand     1\n",
              "Jury      1\n",
              "The       1\n",
              "an        1\n",
              "said      1\n",
              "dtype: int64"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# print an example\n",
        "get_word_frequencies(brown_corpus[0:43])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_vocab(frequencies, vocab_size = 20000):\n",
        "    # sort all values and select the top |v| entries\n",
        "    selection = frequencies.sort_values(ascending = False).iloc[0:vocab_size]\n",
        "    # replace the values with their indices\n",
        "    selection.iloc[:] = range(selection.shape[0])\n",
        "    return selection  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "the     0\n",
              "of      1\n",
              "and     2\n",
              "to      3\n",
              "a       4\n",
              "in      5\n",
              "that    6\n",
              "is      7\n",
              "was     8\n",
              "for     9\n",
              "dtype: int64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# print an example\n",
        "create_vocab(get_word_frequencies(brown_corpus))[0:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def windowizer(corpus):\n",
        "    # split the string into its tokens and turn into iterator\n",
        "    split = iter(corpus.split())\n",
        "    # return the pairs as list\n",
        "    return list(zip(split, split))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('The', 'Fulton'), ('County', 'Grand'), ('Jury', 'said'), ('Friday', 'an')]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# print an example\n",
        "windowizer(brown_corpus[0:43])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5\n",
        "For this task, I choose not to use `sklearn.feature_extraction.text.CountVectorizer` but my own algorithm instead. I also implement some optional preprocessing that turns all words to lower case, and removes specials and stopwords."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def perform_preprocessing(corpus):\n",
        "    import re\n",
        "    # turn the corpus to lower case\n",
        "    corpus = corpus.lower()\n",
        "    # remove all specials\n",
        "    corpus = re.sub(r\"[^A-Za-z0-9\\s\\n]\", \"\", corpus)\n",
        "    # replace single line breaks but not section breaks\n",
        "    corpus = re.sub(r\"(?<!\\n)\\n\", \"\", corpus)\n",
        "    # remove stopwords\n",
        "    from nltk.corpus import stopwords\n",
        "    corpus = re.sub(re.compile(\"|\".join([\"\\s\" + x + \"(?=\\s)\" for x in stopwords.words(\"english\")])), \"\", corpus)\n",
        "    return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_embedding_matrix(corpus):\n",
        "    \n",
        "    # get the vocab mapping from string to integer\n",
        "    vocabulary = create_vocab(get_word_frequencies(corpus))\n",
        "    \n",
        "    # get all grams\n",
        "    grams = windowizer(corpus)\n",
        "\n",
        "    # create a blueprint embedding matrix of all zeros\n",
        "    embedding_matrix = np.zeros((vocabulary.shape[0], vocabulary.shape[0]))\n",
        "    \n",
        "    # iterate over all grams and add to count if both are in vocab\n",
        "    for i in range(len(grams)):\n",
        "        # if one or both are not in vocab a KeyError will be thrown: ignore this error\n",
        "        try:\n",
        "            embedding_matrix[ vocabulary[ grams[i][0] ] ][ vocabulary[ grams[i][1] ] ] += 1\n",
        "            embedding_matrix[ vocabulary[ grams[i][1] ] ][ vocabulary[ grams[i][0] ] ] += 1\n",
        "        except KeyError:\n",
        "            pass\n",
        "        \n",
        "    return embedding_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0.]])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# an example not using preprocessing\n",
        "create_embedding_matrix(brown_corpus[0:43])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define a function for the cosine similarity\n",
        "def cosine_sim(x, y):\n",
        "    return((x @ y) / ((sum(x ** 2) ** .5) * (sum(y ** 2) ** .5)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def most_similar_words(embedding_matrix, vocabulary, test_word):\n",
        "    \n",
        "    # get the index of the desired word\n",
        "    test_word_index = vocabulary[ test_word ]\n",
        "    \n",
        "    # create a blueprint series for the similarities\n",
        "    similarities = pd.Series(0, index = vocabulary.index)\n",
        "    \n",
        "    # iterate over the vocabulary and calculate similarties between the esired and all other words\n",
        "    for i in range(vocabulary.shape[0]):\n",
        "        similarities[i] = cosine_sim(embedding_matrix[test_word_index], embedding_matrix[i])\n",
        "      \n",
        "    # return the words with highest similarity scores  \n",
        "    return similarities.drop(test_word).sort_values(ascending = False).head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create embeddings and vocabulary w/o preprocessing\n",
        "embedding_matrix = create_embedding_matrix(brown_corpus)\n",
        "vocabulary = create_vocab(get_word_frequencies(brown_corpus))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/scratch/slurm_tmpdir/job_21356974/ipykernel_1098824/1356835564.py:3: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  return((x @ y) / ((sum(x ** 2) ** .5) * (sum(y ** 2) ** .5)))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "selects    0.272772\n",
              "\"same      0.267261\n",
              "invaded    0.239046\n",
              "touring    0.231455\n",
              "glue       0.222718\n",
              "dtype: float64"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# get the most similar words to \"County\"\n",
        "most_similar_words(embedding_matrix, vocabulary, \"County\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create embeddings and vocabulary with preprocessing\n",
        "embedding_matrix = create_embedding_matrix(perform_preprocessing(brown_corpus))\n",
        "vocabulary = create_vocab(get_word_frequencies(perform_preprocessing(brown_corpus)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/scratch/slurm_tmpdir/job_21356974/ipykernel_1098824/1356835564.py:3: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  return((x @ y) / ((sum(x ** 2) ** .5) * (sum(y ** 2) ** .5)))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tex              0.228218\n",
              "superior         0.225000\n",
              "supreme          0.222159\n",
              "parkhouse        0.218218\n",
              "desegregation    0.217597\n",
              "dtype: float64"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# again get the most similar words to \"county\" (in lower case this time)\n",
        "most_similar_words(embedding_matrix, vocabulary, \"county\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
